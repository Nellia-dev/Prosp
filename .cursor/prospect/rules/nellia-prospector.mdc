---
description: Core Nellia Prospector project guidelines and architecture
globs: 
alwaysApply: true
---
# Nellia Prospector - AI-Powered B2B Lead Processing System

## Project Overview

Nellia Prospector is a sophisticated multi-agent system that processes B2B leads through an AI-powered pipeline. The system takes raw lead data from a "Harvester" service and transforms it into actionable, personalized outreach opportunities with a claimed 527% ROI.

**Key Documentation:**
- Project idealization: [project-idealization.md](mdc:project-idealization.md)
- Implementation TODO: [NELLIA_PROSPECTOR_TODO.md](mdc:NELLIA_PROSPECTOR_TODO.md)
- Main README: [README.md](mdc:README.md)

## Architecture & Design Principles

### Multi-Agent Pipeline Architecture
The system follows a sequential agent pipeline pattern:
```
Harvester Output → Lead Intake & Validation → Lead Analysis → Persona Creation → Approach Strategy → Message Crafting → Final Output
```

Each agent:
- Has a single responsibility
- Inherits from [BaseAgent](mdc:agents/base_agent.py)
- Uses Pydantic models for input/output validation
- Includes error handling and retry logic
- Tracks metrics and performance

### Code Organization
```
nellia_prospector/
├── [main.py](mdc:main.py)                    # Main orchestrator with CLI
├── agents/                                   # Agent implementations
│   ├── [base_agent.py](mdc:agents/base_agent.py)
│   ├── [lead_intake_agent.py](mdc:agents/lead_intake_agent.py)
│   └── [lead_analysis_agent.py](mdc:agents/lead_analysis_agent.py)
├── core_logic/                               # Core business logic
│   └── [llm_client.py](mdc:core_logic/llm_client.py)
├── data_models/                              # Pydantic models
│   └── [lead_structures.py](mdc:data_models/lead_structures.py)
└── harvester_output/                         # Sample data
```

## Data Models & Flow

### Core Data Models
All data models are defined in [lead_structures.py](mdc:data_models/lead_structures.py):

1. **Input Models:**
   - `HarvesterOutput`: Raw output from harvester service
   - `SiteData`: Individual lead website data
   
2. **Processing Models:**
   - `ValidatedLead`: Validated and cleaned lead
   - `AnalyzedLead`: Lead with analysis results
   - `LeadWithPersona`: Lead with decision-maker persona
   - `LeadWithStrategy`: Lead with approach strategy
   - `FinalProspectPackage`: Complete processed lead

3. **Future A2A Models:**
   - `A2AAgentMessage`: Base for Google A2A Protocol messages
   - Various message types for distributed agent communication

## Agent Implementation Guidelines

### Creating New Agents
When implementing new agents (Persona, Strategy, Message):

1. **Inherit from BaseAgent:**
```python
from agents.base_agent import BaseAgent
from data_models.lead_structures import InputModel, OutputModel

class NewAgent(BaseAgent[InputModel, OutputModel]):
    def process(self, input_data: InputModel) -> OutputModel:
        # Implementation
```

2. **Use the LLM Client:**
```python
result = self.llm_client.generate(
    prompt=prompt,
    temperature=0.7,
    max_tokens=8192
)
```

3. **Handle Errors Gracefully:**
- Use try-except blocks
- Log errors with context
- Return meaningful error states

4. **Track Metrics:**
- Processing time
- Token usage
- Success/failure rates

### Current Agent Status
- ✅ Lead Intake & Validation Agent - Complete
- ✅ Lead Analysis Agent - Complete  
- ❌ Persona Creation Agent - To be ported from [cw.py](mdc:cw.py)
- ❌ Approach Strategy Agent - To be ported from [cw.py](mdc:cw.py)
- ❌ Message Crafting Agent - To be ported from [cw.py](mdc:cw.py)

## LLM Integration

The system uses a flexible LLM client ([llm_client.py](mdc:core_logic/llm_client.py)) that supports:
- **Google Gemini** (primary)
- **OpenAI GPT** (optional)

Key features:
- Automatic retry with exponential backoff
- Token usage tracking
- Error handling for rate limits
- Abstract interface for easy provider switching

## Environment Configuration

Required environment variables (see [env.example](mdc:env.example)):
```bash
GEMINI_API_KEY=your_key_here      # Required
OPENAI_API_KEY=your_key_here      # Optional
AGENT_TEMPERATURE=0.7
AGENT_MAX_TOKENS=8192
```

## Testing & Quality

### Running the System
```bash
python main.py harvester_output/example.json -p "Your product/service"
```

### Code Style
- Use Black for formatting: `black .`
- Follow PEP 8 guidelines
- Type hints required for all functions
- Comprehensive docstrings for classes and methods

### Testing Requirements
- Unit tests for each agent
- Integration tests for full pipeline
- Mock LLM calls in tests
- Minimum 80% code coverage

## Future Enhancements

### Google A2A Protocol Integration
The project is designed for future integration with [Google's Agent2Agent Protocol](mdc:https:/github.com/google-a2a/A2A):
- See detailed guide: [A2A Integration Guide](mdc:docs/A2A_INTEGRATION_GUIDE.md)
- Would enable distributed agent deployment
- Each agent becomes an independent service
- Allows integration with third-party A2A agents

### Planned Features
- CrewAI integration for agent orchestration
- Advanced NLP with LangChain
- Real-time processing capabilities
- Multi-language support
- Analytics dashboard

## Important Patterns & Conventions

### Error Handling
```python
try:
    result = await self.process_lead(lead)
except LLMError as e:
    self.logger.error(f"LLM error: {e}")
    return self.create_error_response(lead, str(e))
```

### Logging
Use loguru for structured logging:
```python
logger.info("Processing lead", url=lead.url, status="started")
```

### Rich Terminal UI
The system uses Rich for beautiful terminal output:
- Progress bars for processing
- Tables for results
- Colored status indicators

## Common Tasks

### Adding a New Agent
1. Create agent file in `agents/`
2. Inherit from `BaseAgent`
3. Define input/output models in `data_models/lead_structures.py`
4. Implement `process()` method
5. Add to pipeline in `main.py`
6. Update progress tracking

### Modifying LLM Prompts
- Keep prompts clear and structured
- Use JSON output format when possible
- Include examples in prompts
- Test with multiple LLM providers

### Debugging Tips
- Enable DEBUG logging: `-l DEBUG`
- Check `agent_metrics` in output JSON
- Use `--limit` to test with fewer leads
- Inspect LLM token usage for cost optimization

## Performance Considerations

- Current processing: ~5-10 seconds per lead
- Target: <3 minutes per lead for full pipeline
- Batch processing supported but sequential
- Future: Parallel processing with A2A

## Security & Compliance

- LGPD compliant data processing
- No persistent storage of personal data
- API keys stored in environment variables
- Future: OAuth2 for A2A authentication

## Contact & Support

- Email: contato@nellia.com.br
- WhatsApp: (11) 98640-9993
- Website: https://prospect.nellia.com.br
- GitHub: Internal repository

## Key Files Reference

- Entry point: [main.py](mdc:main.py)
- Base agent class: [agents/base_agent.py](mdc:agents/base_agent.py)
- Data models: [data_models/lead_structures.py](mdc:data_models/lead_structures.py)
- LLM client: [core_logic/llm_client.py](mdc:core_logic/llm_client.py)
- Legacy implementation: [cw.py](mdc:cw.py) (reference for porting)
- Harvester: [harvester.py](mdc:harvester.py) (data collection tool)

Remember: The goal is to achieve 527% ROI through intelligent, personalized B2B lead processing!
