from typing import Optional, List, Dict
from pydantic import BaseModel, Field

from .base_agent import BaseAgent
from core_logic.llm_client import LLMClientBase, LLMResponse

# Constants
GEMINI_TEXT_INPUT_TRUNCATE_CHARS = 180000

class StrategicQuestionGenerationInput(BaseModel):
    lead_analysis: str
    persona_profile: str
    deepened_pain_points: str # JSON string from PainPointDeepeningAgent
    # product_service_info: Optional[str] = None # Not currently used in prompt
    # value_propositions: Optional[List[str]] = Field(default_factory=list) # Not currently used in prompt


# Updated Pydantic Output Model
class StrategicQuestionGenerationOutput(BaseModel):
    generated_questions: List[str] = Field(default_factory=list, description="Lista de 3-5 perguntas estrat√©gicas, abertas.")
    question_category_map: Dict[str, str] = Field(default_factory=dict,
                                                  description="Mapeia cada pergunta gerada √† sua categoria estrat√©gica (ex: 'Desafio Principal', 'Vis√£o de Futuro').")
    overall_questioning_strategy_summary: Optional[str] = Field(default=None,
                                                               description="Breve resumo da l√≥gica ou objetivo estrat√©gico por tr√°s do conjunto de perguntas formuladas.")
    error_message: Optional[str] = None

class StrategicQuestionGenerationAgent(BaseAgent[StrategicQuestionGenerationInput, StrategicQuestionGenerationOutput]):
    def __init__(self, llm_client: Optional[LLMClientBase] = None, **kwargs):
        super().__init__(
            name="StrategicQuestionGenerationAgent",
            description="Generates strategic, open-ended questions to deepen lead discovery.",
            llm_client=llm_client,
            **kwargs
        )

    def _truncate_text(self, text: str, max_chars: int) -> str:
        """Truncates text to a maximum number of characters."""
        return text[:max_chars]

    def process(self, input_data: StrategicQuestionGenerationInput) -> StrategicQuestionGenerationOutput:
        error_message = None
        self.logger.info(f"‚ùì STRATEGIC QUESTION GENERATION AGENT STARTING...")
        self.logger.debug(f"üìä Input data: analysis_len={len(input_data.lead_analysis)}, persona_len={len(input_data.persona_profile)}, pain_points_len={len(input_data.deepened_pain_points)}")

        try:
            # Truncate inputs
            prompt_fixed_overhead = 4000 # Estimate for fixed parts of the prompt and JSON structure
            available_for_dynamic = GEMINI_TEXT_INPUT_TRUNCATE_CHARS - prompt_fixed_overhead

            tr_lead_analysis = self._truncate_text(input_data.lead_analysis, int(available_for_dynamic * 0.30))
            tr_persona_profile = self._truncate_text(input_data.persona_profile, int(available_for_dynamic * 0.30))
            # Ensure deepened_pain_points (JSON string) is also truncated if very long
            tr_deepened_pain_points = self._truncate_text(input_data.deepened_pain_points, int(available_for_dynamic * 0.40))

            # Refined prompt_template
            prompt_template = """
                Voc√™ √© um Coach de Vendas Estrat√©gicas e Especialista em Discovery Calls, com profundo conhecimento do mercado B2B brasileiro.
                Sua miss√£o √© formular de 3 a 5 perguntas estrat√©gicas, abertas e instigantes, que v√£o AL√âM das perguntas investigativas j√° contidas nos "PONTOS DE DOR J√Å MAPEADOS".
                Estas novas perguntas devem ajudar um vendedor a:
                -   Validar e aprofundar a compreens√£o das necessidades e desafios da persona.
                -   Incentivar a persona a refletir sobre a vis√£o de longo prazo da empresa.
                -   Explorar as implica√ß√µes mais amplas dos desafios atuais (al√©m do impacto imediato).
                -   Descobrir objetivos, aspira√ß√µes ou preocupa√ß√µes ainda n√£o explicitamente mencionadas.
                -   Preparar o terreno para apresentar solu√ß√µes de forma consultiva.
                -   Serem formuladas considerando as nuances da comunica√ß√£o empresarial no Brasil (ex: respeito, busca por clareza, constru√ß√£o de relacionamento).

                CONTEXTO DISPON√çVEL:

                1. AN√ÅLISE DO LEAD:
                   \"\"\"
                   {lead_analysis}
                   \"\"\"

                2. PERFIL DA PERSONA (Tomador de Decis√£o):
                   \"\"\"
                   {persona_profile}
                   \"\"\"

                3. PONTOS DE DOR J√Å MAPEADOS (E PERGUNTAS INVESTIGATIVAS INICIAIS ASSOCIADAS A ELES):
                   \"\"\"
                   {deepened_pain_points}
                   \"\"\"

                INSTRU√á√ïES PARA AS PERGUNTAS ESTRAT√âGICAS:
                1.  Revise TODO o contexto para identificar lacunas de informa√ß√£o ou √°reas para explora√ß√£o mais profunda.
                2.  N√ÉO REPITA ou reformule superficialmente as perguntas j√° existentes nos "PONTOS DE DOR J√Å MAPEADOS". Crie perguntas genuinamente NOVAS e mais ESTRAT√âGICAS.
                3.  As perguntas devem ser ABERTAS (incentivando respostas elaboradas, n√£o "sim/n√£o"). Use preferencialmente "Como...", "Quais s√£o os impactos de...", "De que forma...", "Qual sua vis√£o sobre...". Evite perguntas que comecem com "Voc√™ acha que..." ou que sugiram a resposta.
                4.  As perguntas devem ser NEUTRAS, focadas no cliente, e n√£o devem vender explicitamente nenhuma solu√ß√£o neste momento.
                5.  Para cada pergunta em `generated_questions`, atribua uma categoria correspondente em `question_category_map` que justifique seu prop√≥sito. Exemplos de Categorias: "Desafio Principal e Impacto", "Processo de Decis√£o e Influenciadores", "Vis√£o de Futuro e Metas de Longo Prazo", "Crit√©rios para Solu√ß√£o Ideal", "Riscos e Preocupa√ß√µes Atuais", "Ambiente Competitivo e Diferenciais", "Crit√©rios de Sucesso e M√©tricas".
                6.  Forne√ßa um breve resumo da estrat√©gia geral por tr√°s do conjunto de perguntas que voc√™ formulou no campo `overall_questioning_strategy_summary`.

                FORMATO DA RESPOSTA:
                Responda EXCLUSIVAMENTE com um objeto JSON v√°lido, seguindo o schema e as descri√ß√µes de campo abaixo. N√£o inclua NENHUM texto, explica√ß√£o, ou markdown (como ```json) antes ou depois do objeto JSON.

                SCHEMA JSON ESPERADO:
                {{
                  "generated_questions": [
                    "string - Primeira pergunta estrat√©gica aberta e concisa.",
                    "string - Segunda pergunta estrat√©gica aberta e concisa."
                    // Inclua de 3 a 5 perguntas no total.
                  ],
                  "question_category_map": {{ // Dicion√°rio mapeando CADA pergunta em "generated_questions" √† sua categoria. A chave deve ser a pergunta EXATA.
                    "Primeira pergunta estrat√©gica aberta e concisa.": "string - Categoria da Pergunta (ex: Desafio Principal e Impacto, Vis√£o de Futuro e Metas de Longo Prazo)",
                    "Segunda pergunta estrat√©gica aberta e concisa.": "string - Categoria da Pergunta"
                    // ... e assim por diante para cada pergunta em "generated_questions".
                  }},
                  "overall_questioning_strategy_summary": "string | null - Breve resumo (1-2 frases) da l√≥gica ou objetivo estrat√©gico por tr√°s do conjunto de perguntas formuladas (o que se espera descobrir ou validar com elas). Use null se n√£o houver um resumo espec√≠fico ou se for autoexplicativo."
                }}
            """

            formatted_prompt = prompt_template.format(
                lead_analysis=tr_lead_analysis,
                persona_profile=tr_persona_profile,
                deepened_pain_points=tr_deepened_pain_points
            )
            self.logger.debug(f"Prompt for {self.name} (length: {len(formatted_prompt)}):\n{formatted_prompt[:600]}...")

            llm_response: Optional[LLMResponse] = self.generate_llm_response(formatted_prompt)

            if not llm_response or not llm_response.content:
                self.logger.error(f"‚ùå LLM call returned no response for {self.name}")
                return StrategicQuestionGenerationOutput(error_message="LLM call returned no response.")

            llm_response_str = llm_response.content
            self.logger.debug(f"LLM response received for {self.name} (length: {len(llm_response_str)}). Attempting to parse.")
            parsed_output = self.parse_llm_json_response(llm_response_str, StrategicQuestionGenerationOutput)
            
            if parsed_output.error_message:
                 self.logger.warning(f"‚ö†Ô∏è {self.name} JSON parsing/validation failed. Error: {parsed_output.error_message}. Raw response snippet: {llm_response_str[:500]}")
                 return parsed_output # Return object with error message set

            # Additional validation for question_category_map consistency
            if parsed_output.generated_questions and not parsed_output.question_category_map:
                self.logger.warning(f"‚ö†Ô∏è {self.name}: LLM generated questions but question_category_map is empty. Questions might lack categorization.")
            elif parsed_output.generated_questions and (len(parsed_output.question_category_map) != len(parsed_output.generated_questions) or \
                 not all(q in parsed_output.question_category_map for q in parsed_output.generated_questions)):
                 self.logger.warning(f"‚ö†Ô∏è {self.name}: Mismatch or inconsistency between generated_questions and question_category_map keys. LLM output may need review.")
                 # Potentially set an error or try to reconcile, but for now, just log.

            self.logger.info(f"‚úÖ Strategic questions generated by {self.name}: {len(parsed_output.generated_questions)} questions.")
            return parsed_output

        except Exception as e:
            self.logger.error(f"‚ùå An unexpected error occurred in {self.name}: {e}", exc_info=True)
            return StrategicQuestionGenerationOutput(error_message=f"An unexpected error occurred: {str(e)}")

