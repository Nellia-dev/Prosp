from typing import Optional, List, Dict
from pydantic import BaseModel, Field
import json

from agents.base_agent import BaseAgent
from core_logic.llm_client import LLMClientBase

# Constants
GEMINI_TEXT_INPUT_TRUNCATE_CHARS = 180000

class StrategicQuestionGenerationInput(BaseModel):
    lead_analysis: str
    persona_profile: str
    deepened_pain_points: str # JSON string from PainPointDeepeningAgent
    # product_service_info: Optional[str] = None # Not currently used in prompt
    # value_propositions: Optional[List[str]] = Field(default_factory=list) # Not currently used in prompt


# Updated Pydantic Output Model
class StrategicQuestionGenerationOutput(BaseModel):
    generated_questions: List[str] = Field(default_factory=list, description="Lista de 3-5 perguntas estrat√©gicas, abertas.")
    question_category_map: Dict[str, str] = Field(default_factory=dict,
                                                  description="Mapeia cada pergunta gerada √† sua categoria estrat√©gica (ex: 'Desafio Principal', 'Vis√£o de Futuro').")
    overall_questioning_strategy_summary: Optional[str] = Field(default=None,
                                                               description="Breve resumo da l√≥gica ou objetivo estrat√©gico por tr√°s do conjunto de perguntas formuladas.")
    error_message: Optional[str] = None

class StrategicQuestionGenerationAgent(BaseAgent[StrategicQuestionGenerationInput, StrategicQuestionGenerationOutput]):
    def __init__(self, name: str, description: str, llm_client: LLMClientBase, **kwargs):
        super().__init__(name=name, description=description, llm_client=llm_client, **kwargs)

    def _truncate_text(self, text: str, max_chars: int) -> str:
        """Truncates text to a maximum number of characters."""
        return text[:max_chars]

    def process(self, input_data: StrategicQuestionGenerationInput) -> StrategicQuestionGenerationOutput:
        error_message = None
        self.logger.info(f"‚ùì STRATEGIC QUESTION GENERATION AGENT STARTING...")
        self.logger.debug(f"üìä Input data: analysis_len={len(input_data.lead_analysis)}, persona_len={len(input_data.persona_profile)}, pain_points_len={len(input_data.deepened_pain_points)}")

        try:
            # Truncate inputs
            prompt_fixed_overhead = 4000 # Estimate for fixed parts of the prompt and JSON structure
            available_for_dynamic = GEMINI_TEXT_INPUT_TRUNCATE_CHARS - prompt_fixed_overhead

            tr_lead_analysis = self._truncate_text(input_data.lead_analysis, int(available_for_dynamic * 0.30))
            tr_persona_profile = self._truncate_text(input_data.persona_profile, int(available_for_dynamic * 0.30))
            # Ensure deepened_pain_points (JSON string) is also truncated if very long
            tr_deepened_pain_points = self._truncate_text(input_data.deepened_pain_points, int(available_for_dynamic * 0.40))

            # Refined prompt_template
            prompt_template = """
                Voc√™ √© um Coach de Vendas Estrat√©gicas e Especialista em Discovery Calls, com profundo conhecimento do mercado B2B brasileiro.
                Sua miss√£o √© formular de 3 a 5 perguntas estrat√©gicas, abertas e instigantes, que v√£o AL√âM das perguntas investigativas j√° contidas nos "PONTOS DE DOR J√Å MAPEADOS".
                Estas novas perguntas devem ajudar um vendedor a:
                -   Validar e aprofundar a compreens√£o das necessidades e desafios da persona.
                -   Incentivar a persona a refletir sobre a vis√£o de longo prazo da empresa.
                -   Explorar as implica√ß√µes mais amplas dos desafios atuais (al√©m do impacto imediato).
                -   Descobrir objetivos, aspira√ß√µes ou preocupa√ß√µes ainda n√£o explicitamente mencionadas.
                -   Preparar o terreno para apresentar solu√ß√µes de forma consultiva.
                -   Serem formuladas considerando as nuances da comunica√ß√£o empresarial no Brasil (ex: respeito, busca por clareza, constru√ß√£o de relacionamento).

                CONTEXTO DISPON√çVEL:

                1. AN√ÅLISE DO LEAD:
                   \"\"\"
                   {lead_analysis}
                   \"\"\"

                2. PERFIL DA PERSONA (Tomador de Decis√£o):
                   \"\"\"
                   {persona_profile}
                   \"\"\"

                3. PONTOS DE DOR J√Å MAPEADOS (E PERGUNTAS INVESTIGATIVAS INICIAIS ASSOCIADAS A ELES):
                   \"\"\"
                   {deepened_pain_points}
                   \"\"\"

                INSTRU√á√ïES PARA AS PERGUNTAS ESTRAT√âGICAS:
                1.  Revise TODO o contexto para identificar lacunas de informa√ß√£o ou √°reas para explora√ß√£o mais profunda.
                2.  N√ÉO REPITA ou reformule superficialmente as perguntas j√° existentes nos "PONTOS DE DOR J√Å MAPEADOS". Crie perguntas genuinamente NOVAS e mais ESTRAT√âGICAS.
                3.  As perguntas devem ser ABERTAS (incentivando respostas elaboradas, n√£o "sim/n√£o"). Use preferencialmente "Como...", "Quais s√£o os impactos de...", "De que forma...", "Qual sua vis√£o sobre...". Evite perguntas que comecem com "Voc√™ acha que..." ou que sugiram a resposta.
                4.  As perguntas devem ser NEUTRAS, focadas no cliente, e n√£o devem vender explicitamente nenhuma solu√ß√£o neste momento.
                5.  Para cada pergunta em `generated_questions`, atribua uma categoria correspondente em `question_category_map` que justifique seu prop√≥sito. Exemplos de Categorias: "Desafio Principal e Impacto", "Processo de Decis√£o e Influenciadores", "Vis√£o de Futuro e Metas de Longo Prazo", "Crit√©rios para Solu√ß√£o Ideal", "Riscos e Preocupa√ß√µes Atuais", "Ambiente Competitivo e Diferenciais", "Crit√©rios de Sucesso e M√©tricas".
                6.  Forne√ßa um breve resumo da estrat√©gia geral por tr√°s do conjunto de perguntas que voc√™ formulou no campo `overall_questioning_strategy_summary`.

                FORMATO DA RESPOSTA:
                Responda EXCLUSIVAMENTE com um objeto JSON v√°lido, seguindo o schema e as descri√ß√µes de campo abaixo. N√£o inclua NENHUM texto, explica√ß√£o, ou markdown (como ```json) antes ou depois do objeto JSON.

                SCHEMA JSON ESPERADO:
                {{
                  "generated_questions": [
                    "string - Primeira pergunta estrat√©gica aberta e concisa.",
                    "string - Segunda pergunta estrat√©gica aberta e concisa."
                    // Inclua de 3 a 5 perguntas no total.
                  ],
                  "question_category_map": {{ // Dicion√°rio mapeando CADA pergunta em "generated_questions" √† sua categoria. A chave deve ser a pergunta EXATA.
                    "Primeira pergunta estrat√©gica aberta e concisa.": "string - Categoria da Pergunta (ex: Desafio Principal e Impacto, Vis√£o de Futuro e Metas de Longo Prazo)",
                    "Segunda pergunta estrat√©gica aberta e concisa.": "string - Categoria da Pergunta"
                    // ... e assim por diante para cada pergunta em "generated_questions".
                  }},
                  "overall_questioning_strategy_summary": "string | null - Breve resumo (1-2 frases) da l√≥gica ou objetivo estrat√©gico por tr√°s do conjunto de perguntas formuladas (o que se espera descobrir ou validar com elas). Use null se n√£o houver um resumo espec√≠fico ou se for autoexplicativo."
                }}
            """

            formatted_prompt = prompt_template.format(
                lead_analysis=tr_lead_analysis,
                persona_profile=tr_persona_profile,
                deepened_pain_points=tr_deepened_pain_points
            )
            self.logger.debug(f"Prompt for {self.name} (length: {len(formatted_prompt)}):\n{formatted_prompt[:600]}...")

            llm_response_str = self.generate_llm_response(formatted_prompt)

            if not llm_response_str:
                self.logger.error(f"‚ùå LLM call returned no response for {self.name}")
                return StrategicQuestionGenerationOutput(error_message="LLM call returned no response.")

            self.logger.debug(f"LLM response received for {self.name} (length: {len(llm_response_str)}). Attempting to parse.")
            parsed_output = self.parse_llm_json_response(llm_response_str, StrategicQuestionGenerationOutput)
            
            if parsed_output.error_message:
                 self.logger.warning(f"‚ö†Ô∏è {self.name} JSON parsing/validation failed. Error: {parsed_output.error_message}. Raw response snippet: {llm_response_str[:500]}")
                 return parsed_output # Return object with error message set

            # Additional validation for question_category_map consistency
            if parsed_output.generated_questions and not parsed_output.question_category_map:
                self.logger.warning(f"‚ö†Ô∏è {self.name}: LLM generated questions but question_category_map is empty. Questions might lack categorization.")
            elif parsed_output.generated_questions and (len(parsed_output.question_category_map) != len(parsed_output.generated_questions) or \
                 not all(q in parsed_output.question_category_map for q in parsed_output.generated_questions)):
                 self.logger.warning(f"‚ö†Ô∏è {self.name}: Mismatch or inconsistency between generated_questions and question_category_map keys. LLM output may need review.")
                 # Potentially set an error or try to reconcile, but for now, just log.

            self.logger.info(f"‚úÖ Strategic questions generated by {self.name}: {len(parsed_output.generated_questions)} questions.")
            return parsed_output

        except Exception as e:
            self.logger.error(f"‚ùå An unexpected error occurred in {self.name}: {e}", exc_info=True)
            return StrategicQuestionGenerationOutput(error_message=f"An unexpected error occurred: {str(e)}")

if __name__ == '__main__':
    from loguru import logger
    import sys
    logger.remove()
    logger.add(sys.stderr, level="DEBUG")

    class MockLLMClient(LLMClientBase):
        def __init__(self, api_key: str = "mock_key"):
            super().__init__(api_key)

        def generate_text_response(self, prompt: str) -> Optional[str]:
            logger.debug(f"MockLLMClient received prompt snippet:\n{prompt[:600]}...")

            q1 = "Considerando a expans√£o para LATAM, qual seria o impacto em 3 anos na estrutura de custos e na receita da Empresa Exemplo se os atuais desafios de otimiza√ß√£o de processos de QA n√£o forem endere√ßados com novas tecnologias?"
            q2 = "Al√©m da efici√™ncia operacional, quais outras m√©tricas de sucesso s√£o cruciais para Carlos Mendes ao avaliar o impacto de novas solu√ß√µes tecnol√≥gicas na √°rea de DevOps?"
            q3 = "De que forma a capacidade de inova√ß√£o da Empresa Exemplo poderia ser potencializada se a equipe de desenvolvimento e QA tivesse mais tempo liberado de tarefas manuais repetitivas?"

            return json.dumps({
                "generated_questions": [q1, q2, q3],
                "question_category_map": {
                    q1: "Vis√£o de Futuro e Impacto Financeiro",
                    q2: "Crit√©rios de Sucesso e M√©tricas",
                    q3: "Impacto da Solu√ß√£o/Benef√≠cios Esperados"
                },
                "overall_questioning_strategy_summary": "As perguntas visam explorar as implica√ß√µes de longo prazo dos desafios atuais, os crit√©rios de sucesso da persona e o potencial de inova√ß√£o desbloqueado pela solu√ß√£o."
            })

    logger.info("Running mock test for StrategicQuestionGenerationAgent...")
    mock_llm = MockLLMClient(api_key="mock_llm_key")
    agent = StrategicQuestionGenerationAgent(
        name="TestStrategicQuestionAgent",
        description="Test Agent for Strategic Question Generation",
        llm_client=mock_llm
    )

    test_lead_analysis = "A Empresa Exemplo (m√©dio porte, TI) foca em otimiza√ß√£o de processos de desenvolvimento para suportar sua expans√£o LATAM. Recentemente recebeu investimento S√©rie B."
    test_persona_profile = "Carlos Mendes, Diretor de Opera√ß√µes. Respons√°vel por efici√™ncia operacional e ado√ß√£o de novas tecnologias. Busca ROI claro e integra√ß√£o facilitada."
    test_deepened_pain_points = json.dumps({
        "primary_pain_category": "Efici√™ncia Operacional",
        "detailed_pain_points": [{
            "pain_point_title": "Otimiza√ß√£o de Processos Manuais",
            "detailed_description": "Processos manuais em QA est√£o causando lentid√£o.",
            "potential_business_impact": "Atrasos nos lan√ßamentos.",
            "how_our_solution_helps": "Nossa IA automatiza QA.",
            "investigative_questions": ["Como os processos manuais atuais impactam o tempo?"]
        }],
        "urgency_level": "high",
        "overall_pain_summary": "Necessidade clara de automa√ß√£o para escalar."
    })

    input_data = StrategicQuestionGenerationInput(
        lead_analysis=test_lead_analysis,
        persona_profile=test_persona_profile,
        deepened_pain_points=test_deepened_pain_points
    )

    output = agent.process(input_data)

    if output.error_message:
        logger.error(f"Error: {output.error_message}")
    else:
        logger.success("StrategicQuestionGenerationAgent processed successfully.")
        logger.info(f"Generated Questions ({len(output.generated_questions)}):")
        for q in output.generated_questions:
            logger.info(f"  - Q: {q}")
            logger.info(f"    Category: {output.question_category_map.get(q)}")
        logger.info(f"Strategy Summary: {output.overall_questioning_strategy_summary}")


    assert output.error_message is None
    assert len(output.generated_questions) == 3
    assert output.generated_questions[0] in output.question_category_map
    assert "evolu√ß√£o da otimiza√ß√£o de processos" in output.generated_questions[0] # Check partial content of a question
    assert output.overall_questioning_strategy_summary is not None
    assert "longo prazo" in output.overall_questioning_strategy_summary.lower()

    logger.info("\nMock test for StrategicQuestionGenerationAgent completed successfully.")

```
